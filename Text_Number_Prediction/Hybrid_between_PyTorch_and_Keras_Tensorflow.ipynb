{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hybrid between PyTorch and Keras Tensorflow",
      "provenance": [],
      "collapsed_sections": [
        "tUdsNwFSFSED",
        "A4iNmEGzoEFN"
      ],
      "authorship_tag": "ABX9TyM1DVUYHPq2lG/zbeANgk19",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sangeetsaurabh/PyTorch_Keras_Experiment/blob/master/Text_Number_Prediction/Hybrid_between_PyTorch_and_Keras_Tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8_nrruGfx2a",
        "colab_type": "text"
      },
      "source": [
        "## Hybrid Keras and Tensorflow Model\n",
        "\n",
        "In this notebook, embeddings are generated using Keras model. Pytorch LSTM model uses Keras generated embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUdsNwFSFSED",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "#### Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "149iYfEnjyLR",
        "colab_type": "code",
        "outputId": "6b08bd3a-c511-4664-e964-78a653e3596c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "#### Make sure that the right version of Torch is there\n",
        "!pip install torchtext==0.6.0\n",
        "import torchtext\n",
        "print(torchtext.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchtext==0.6.0 in /usr/local/lib/python3.6/dist-packages (0.6.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6.0) (0.1.90)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6.0) (1.5.0+cu101)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6.0) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6.0) (1.12.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6.0) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6.0) (1.18.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.6.0) (0.16.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.6.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.6.0) (2020.4.5.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.6.0) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.6.0) (1.24.3)\n",
            "0.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RR4eRGbbjJHU",
        "colab_type": "code",
        "outputId": "5a748d72-b8e0-43d0-f01a-6d464e1fada2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYx3rbtw0NBL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#drive.flush_and_unmount"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBaVXPQZuWLV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#### Setting up the right seed to make Keras result more consistent\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import random as python_random\n",
        "\n",
        "# The below is necessary for starting Numpy generated random numbers\n",
        "# in a well-defined initial state.\n",
        "np.random.seed(123)\n",
        "\n",
        "# The below is necessary for starting core Python generated random numbers\n",
        "# in a well-defined state.\n",
        "python_random.seed(123)\n",
        "\n",
        "# The below set_seed() will make random number generation\n",
        "# in the TensorFlow backend have a well-defined initial state.\n",
        "# For further details, see:\n",
        "# https://www.tensorflow.org/api_docs/python/tf/random/set_seed\n",
        "tf.random.set_seed(1234)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRJv3gWMi6g-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#### Setting up path to import important data preparation Python module\n",
        "import sys\n",
        "import os\n",
        "sys.path.append('/content/drive/My Drive/Colab Notebooks/torch_pipe/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItOt_tGZi6hJ",
        "colab_type": "code",
        "outputId": "b4a63001-29ab-44ce-d6d2-4263aafd5c6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "os.getcwd()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bt_V_qOIi6hR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#### Using torch utilities to prepare the features. Importing all the important files\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from Util.human_language_modeling import *\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "import logging"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iRyHrqii6hY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#### Enabling logging\n",
        "import logging\n",
        "logger = logging.getLogger()\n",
        "fhandler = logging.FileHandler(filename='mylog.log', mode='a')\n",
        "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "fhandler.setFormatter(formatter)\n",
        "logger.addHandler(fhandler)\n",
        "logger.setLevel(logging.DEBUG)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4Gzgd_Ei6hf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#### Setting up the batch size and length of the sequence\n",
        "BATCH_SIZE = 64 ## defining the batch size\n",
        "bptt = 20 ## back propogration through LSTM\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srAakqVfi6hy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### A simplie python function to show text given an array of vectors\n",
        "def show_text(input_vector):\n",
        "    separator = ' '\n",
        "    txt = separator.join([vocab.itos[i] for i in input_vector])\n",
        "    return txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lx9BH9nejF8c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NLP = spacy.load('en_core_web_sm')\n",
        "def tokenizer(comment):\n",
        "    #comment = re.sub(\n",
        "    #    r\"[\\*\\\"“”\\n\\\\…\\+\\-\\/\\=\\(\\)‘•:\\[\\]\\|’\\!;\\.]\", \" \", \n",
        "    #    str(comment))\n",
        "    comment = re.sub(r\"[ ]+\", \" \", comment)\n",
        "    comment = re.sub(r\"\\!+\", \"!\", comment)\n",
        "    comment = re.sub(r\"\\,+\", \",\", comment)\n",
        "    comment = re.sub(r\"\\?+\", \"?\", comment)\n",
        "    #cleanr = re.compile('<.*?>')\n",
        "    #cleanr = re.compile('<>')\n",
        "    #comment = re.sub(cleanr, '', comment)\n",
        "    #if (len(comment) > MAX_CHARS):\n",
        "    #   comment = comment[:MAX_CHARS]\n",
        "    return[x.text for x in NLP.tokenizer(comment) if x.text != \" \"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-MF6hGBi6iB",
        "colab_type": "code",
        "outputId": "094c2da8-dcee-464b-a4fa-029eb143bdca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "tokenizer = get_tokenizer(\"spacy\")\n",
        "train_dataset, valid_dataset = HumanNumbers(root='data',bptt=bptt,batch_size=BATCH_SIZE,data_select=('train', 'valid'))\n",
        "vocab = train_dataset.get_vocab()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3428lines [00:00, 34273.09lines/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "<function tokenizer at 0x7faeded15840>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "8001lines [00:00, 33302.93lines/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "51200\n",
            "51200\n",
            "torch.Size([51200, 20])\n",
            "torch.Size([51200, 20])\n",
            "14080\n",
            "14080\n",
            "torch.Size([14080, 20])\n",
            "torch.Size([14080, 20])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4iNmEGzoEFN",
        "colab_type": "text"
      },
      "source": [
        "#### Extract the features for Keras/Tensor Flow implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9I5fFdykqMqd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, GRU, SimpleRNN\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import initializers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRAfySwzmVl1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#### Building input features and lables for machine learning models\n",
        "train_x = train_dataset.input_data.numpy()\n",
        "train_label = train_dataset.label_data.numpy().astype(int)\n",
        "train_y = tf.keras.utils.to_categorical(train_label, num_classes=len(vocab.itos))\n",
        "\n",
        "valid_x = valid_dataset.input_data.numpy()\n",
        "valid_label = valid_dataset.label_data.numpy()\n",
        "valid_y = tf.keras.utils.to_categorical(valid_label, num_classes=len(vocab.itos))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "342a1nWPni1X",
        "colab_type": "code",
        "outputId": "e119f93c-eeea-44cb-d362-008f61a1f689",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "print(train_x.shape)\n",
        "print (train_y.shape)\n",
        "print(valid_x.shape)\n",
        "print(valid_y.shape)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(51200, 20)\n",
            "(51200, 20, 34)\n",
            "(14080, 20)\n",
            "(14080, 20, 34)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLX-gWBXft86",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#### Setting up Keras dataset to feed into machine learning models\n",
        "BUFFER_SIZE = train_x.shape[0] ## Shuffling the data across entire dataset before building the batch\n",
        "\n",
        "train_batch = tf.data.Dataset.from_tensor_slices((train_x, train_y))\n",
        "train_batch = train_batch.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "\n",
        "val_batch = tf.data.Dataset.from_tensor_slices((valid_x, valid_y))\n",
        "val_batch = val_batch.batch(BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbdHPYLFGCHc",
        "colab_type": "text"
      },
      "source": [
        "#### Train the model using Keras bidirectional GRU model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHxhAEA2eoyl",
        "colab_type": "code",
        "outputId": "dfd7462f-4ac8-4430-d94c-ba91b8dfd8e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "model1 = Sequential()\n",
        "model1.add(Embedding(len(vocab.itos), output_dim=64, input_length=train_x.shape[1]))\n",
        "model1.add( Bidirectional(GRU(units=64,return_sequences=True,kernel_initializer=\"zeros\",recurrent_initializer=\"zeros\",)))\n",
        "model1.add(Dense(len(vocab.itos), activation='softmax'))\n",
        "\n",
        "### Compile the model\n",
        "adam = Adam(lr=0.01)\n",
        "model1.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['categorical_accuracy'])\n",
        "\n",
        "#earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto')\n",
        "#history = model.fit(train_x, train_y, epochs=10, batch_size=64, verbose=1,validation_data=(valid_x,valid_y))\n",
        "history = model1.fit(train_batch, epochs=5, verbose=1,validation_data=val_batch)\n",
        "print (model1.summary())\n",
        "output = model1.predict(valid_x)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "800/800 [==============================] - 29s 36ms/step - loss: 0.1583 - categorical_accuracy: 0.9518 - val_loss: 0.0917 - val_categorical_accuracy: 0.9757\n",
            "Epoch 2/5\n",
            "800/800 [==============================] - 29s 36ms/step - loss: 0.0381 - categorical_accuracy: 0.9852 - val_loss: 0.0891 - val_categorical_accuracy: 0.9846\n",
            "Epoch 3/5\n",
            "800/800 [==============================] - 30s 38ms/step - loss: 0.0281 - categorical_accuracy: 0.9885 - val_loss: 0.1083 - val_categorical_accuracy: 0.9817\n",
            "Epoch 4/5\n",
            "800/800 [==============================] - 29s 36ms/step - loss: 0.0204 - categorical_accuracy: 0.9903 - val_loss: 0.1255 - val_categorical_accuracy: 0.9752\n",
            "Epoch 5/5\n",
            "800/800 [==============================] - 29s 36ms/step - loss: 0.0215 - categorical_accuracy: 0.9914 - val_loss: 0.1142 - val_categorical_accuracy: 0.9836\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 20, 64)            2176      \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 20, 128)           49920     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 20, 34)            4386      \n",
            "=================================================================\n",
            "Total params: 56,482\n",
            "Trainable params: 56,482\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qf6hiYp8e6SV",
        "colab_type": "code",
        "outputId": "728a7580-a2b0-4b23-963d-64449c419e61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "### Visualize the embeddings\n",
        "model1.layers[0].get_weights()[0]"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.01746353, -0.03612994,  0.01480493, ..., -0.01985164,\n",
              "         0.03015638, -0.01682599],\n",
              "       [ 0.00664103, -0.02845314,  0.00909234, ...,  0.03347838,\n",
              "         0.01638332,  0.02350162],\n",
              "       [-0.38225704, -0.29808694, -0.23042633, ...,  0.06806397,\n",
              "        -0.31778467,  0.36928326],\n",
              "       ...,\n",
              "       [-0.19148602, -0.18257464,  0.9640643 , ..., -0.43556073,\n",
              "         0.48647377,  0.6472735 ],\n",
              "       [-0.61393994, -0.5060389 , -0.644289  , ...,  1.1192353 ,\n",
              "        -0.7493163 , -0.23655911],\n",
              "       [-0.10578436, -0.18246987,  0.39633074, ..., -0.14235532,\n",
              "         0.34378192,  2.1207764 ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gv_ENtpFiovQ",
        "colab_type": "text"
      },
      "source": [
        "### LSTM model that will use Keras GRU embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "4b3ed6da-2554-4810-fc5a-236af10a9cc5",
        "id": "g7gIk6HjdZdU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "nv = len(vocab.itos)\n",
        "nv"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "34"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "4f040c24-5536-484f-e497-887c658545e0",
        "id": "z03d6TWCdZdX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "nh = 64\n",
        "nh"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2GG3wvhZdZda",
        "colab": {}
      },
      "source": [
        "def generate_batch(batch):\n",
        "    #print(len(batch))\n",
        "    text = []\n",
        "    label = []\n",
        "    for entry in batch:\n",
        "        text.append(entry[0].numpy())\n",
        "        label.append(entry[1].numpy())\n",
        "    return torch.tensor(text), torch.tensor(label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_81VVmei3Dz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BatchNorm1dFlat(nn.BatchNorm1d):\n",
        "    \"`nn.BatchNorm1d`, but first flattens leading dimensions\"\n",
        "    def forward(self, x):\n",
        "        if x.dim()==2: return super().forward(x)\n",
        "        *f,l = x.shape\n",
        "        x = x.contiguous().view(-1,l)\n",
        "        return super().forward(x).view(*f,l)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0seScc0bGhhI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model7(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.i_h = nn.Embedding(nv,nh)\n",
        "        self.i_h.weight.data.copy_(torch.from_numpy(model1.layers[0].get_weights()[0]))\n",
        "        self.i_h.weight.requires_grad = False\n",
        "        self.rnn = nn.LSTM(nh, nh, 2, batch_first=True,bidirectional=True)\n",
        "        self.h_o = nn.Linear(2*nh,nv)\n",
        "        self.bn = BatchNorm1dFlat(2*nh)\n",
        "        self.h = torch.zeros(2*2, BATCH_SIZE, nh)\n",
        "        self.c = torch.zeros(2*2, BATCH_SIZE, nh)\n",
        "        \n",
        "        \n",
        "    def forward(self, x):\n",
        "        res,(h,c) = self.rnn(self.i_h(x), (self.h,self.c))\n",
        "        #print(\"\\n res is: \")\n",
        "        #print (res[0,0,:])\n",
        "        #self.h = h.detach()\n",
        "        return self.h_o(self.bn(res))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ERrCFNgWdZdZ",
        "outputId": "b6883939-e37b-4460-dd95-ae66d3bf204e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "model = Model7().to(device)\n",
        "model"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Model7(\n",
              "  (i_h): Embedding(34, 64)\n",
              "  (rnn): LSTM(64, 64, num_layers=2, batch_first=True, bidirectional=True)\n",
              "  (h_o): Linear(in_features=128, out_features=34, bias=True)\n",
              "  (bn): BatchNorm1dFlat(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAGysPoCdEbl",
        "colab_type": "text"
      },
      "source": [
        "## Run the models\n",
        "\n",
        "Given the pattern of the numbers, bidirectional LSTM and GRU may work best for this situation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ScF2nXLcdZdc",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def train_func(sub_train_):\n",
        "\n",
        "    # Train the model\n",
        "    train_loss = 0\n",
        "    train_acc = 0\n",
        "    sample_tested = 0\n",
        "    #print(len(sub_train_))\n",
        "    data = DataLoader(sub_train_, batch_size=BATCH_SIZE,shuffle=True,\n",
        "                      collate_fn=generate_batch,drop_last=True)\n",
        "    for i, (text, cls) in enumerate(data):\n",
        "        optimizer.zero_grad()\n",
        "        text, cls = text.to(device), cls.to(device)\n",
        "        #print(text.shape)\n",
        "        output = model(text)\n",
        "        #print(output.shape)\n",
        "        #print(cls.shape)\n",
        "        #print(text)\n",
        "        #print(cls)\n",
        "        for i in range(output.shape[1]):\n",
        "            loss = criterion(output[:,i,:], cls[:,i])\n",
        "            train_loss += loss.item()\n",
        "            #print((output[:,i,:].argmax(1) == cls[:,i]).sum().item())\n",
        "            train_acc += (output[:,i,:].argmax(1) == cls[:,i]).sum().item()\n",
        "            sample_tested += output.shape[0]\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        \n",
        "\n",
        "    # Adjust the learning rate\n",
        "    #scheduler.step()\n",
        "    print(train_acc)\n",
        "    print(sample_tested)\n",
        "    return train_loss / (len(sub_train_)*bptt), train_acc / (len(sub_train_)*bptt)\n",
        "\n",
        "def test_func(data_):\n",
        "    loss = 0\n",
        "    acc = 0\n",
        "    data = DataLoader(data_, batch_size=BATCH_SIZE, collate_fn=generate_batch,drop_last=True)\n",
        "    for text, cls in data:\n",
        "        text, cls = text.to(device), cls.to(device)\n",
        "        with torch.no_grad():\n",
        "            output = model(text)\n",
        "            #print(output.shape)\n",
        "            #print(cls.shape)\n",
        "            for i in range(output.shape[1]):\n",
        "                loss = criterion(output[:,i,:], cls[:,i])\n",
        "                loss += loss.item()\n",
        "                acc += (output[:,i,:].argmax(1) == cls[:,i]).sum().item()\n",
        "\n",
        "    return loss / (len(data_)*bptt), acc / (len(data_)*bptt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab_type": "code",
        "outputId": "d87936b0-52c1-4876-f4aa-9c16fe9613e1",
        "id": "NTb-DfgidZde",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        }
      },
      "source": [
        "N_EPOCHS = 10\n",
        "min_valid_loss = float('inf')\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)\n",
        "\n",
        "#### Break test sets into 2 data sets validations and test data set\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "    train_loss, train_acc = train_func(train_dataset)\n",
        "    valid_loss, valid_acc = test_func(valid_dataset)\n",
        "\n",
        "    secs = int(time.time() - start_time)\n",
        "    mins = secs / 60\n",
        "    secs = secs % 60\n",
        "\n",
        "    print('Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(mins, secs))\n",
        "    print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n",
        "    print(f'\\tLoss: {valid_loss:.4f}(valid)\\t|\\tAcc: {valid_acc * 100:.1f}%(valid)')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "608520\n",
            "1024000\n",
            "Epoch: 1  | time in 1 minutes, 2 seconds\n",
            "\tLoss: 0.0305(train)\t|\tAcc: 59.4%(train)\n",
            "\tLoss: 0.0000(valid)\t|\tAcc: 48.0%(valid)\n",
            "712779\n",
            "1024000\n",
            "Epoch: 2  | time in 1 minutes, 2 seconds\n",
            "\tLoss: 0.0264(train)\t|\tAcc: 69.6%(train)\n",
            "\tLoss: 0.0000(valid)\t|\tAcc: 57.4%(valid)\n",
            "736708\n",
            "1024000\n",
            "Epoch: 3  | time in 1 minutes, 2 seconds\n",
            "\tLoss: 0.0263(train)\t|\tAcc: 71.9%(train)\n",
            "\tLoss: 0.0000(valid)\t|\tAcc: 61.2%(valid)\n",
            "815249\n",
            "1024000\n",
            "Epoch: 4  | time in 1 minutes, 2 seconds\n",
            "\tLoss: 0.0201(train)\t|\tAcc: 79.6%(train)\n",
            "\tLoss: 0.0000(valid)\t|\tAcc: 67.2%(valid)\n",
            "823667\n",
            "1024000\n",
            "Epoch: 5  | time in 1 minutes, 2 seconds\n",
            "\tLoss: 0.0191(train)\t|\tAcc: 80.4%(train)\n",
            "\tLoss: 0.0000(valid)\t|\tAcc: 68.3%(valid)\n",
            "800646\n",
            "1024000\n",
            "Epoch: 6  | time in 1 minutes, 2 seconds\n",
            "\tLoss: 0.0222(train)\t|\tAcc: 78.2%(train)\n",
            "\tLoss: 0.0000(valid)\t|\tAcc: 65.0%(valid)\n",
            "818986\n",
            "1024000\n",
            "Epoch: 7  | time in 1 minutes, 2 seconds\n",
            "\tLoss: 0.0209(train)\t|\tAcc: 80.0%(train)\n",
            "\tLoss: 0.0000(valid)\t|\tAcc: 66.6%(valid)\n",
            "809077\n",
            "1024000\n",
            "Epoch: 8  | time in 1 minutes, 2 seconds\n",
            "\tLoss: 0.0222(train)\t|\tAcc: 79.0%(train)\n",
            "\tLoss: 0.0000(valid)\t|\tAcc: 66.6%(valid)\n",
            "809746\n",
            "1024000\n",
            "Epoch: 9  | time in 1 minutes, 2 seconds\n",
            "\tLoss: 0.0233(train)\t|\tAcc: 79.1%(train)\n",
            "\tLoss: 0.0000(valid)\t|\tAcc: 67.0%(valid)\n",
            "801707\n",
            "1024000\n",
            "Epoch: 10  | time in 1 minutes, 3 seconds\n",
            "\tLoss: 0.0265(train)\t|\tAcc: 78.3%(train)\n",
            "\tLoss: 0.0000(valid)\t|\tAcc: 67.4%(valid)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}